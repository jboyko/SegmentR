---
title: "Using the SegmentR Package"
author: "James Boyko"
date: "`r Sys.Date()`"
bibliography: bibliography.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using the SegmentR Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## SegmentR

The SegmentR package is a tool for image segmentation and color extraction. In this vignette, we'll walk through an example of how to use the package's key features. SegmentR is built off of 2 pretrained models: [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) [@liu2023grounding] and [SegmentAnything.](https://github.com/facebookresearch/segment-anything) [@kirillov2023segany]. Specifically, I use a lighter version of SegmentAnything called [SlimSAM](https://github.com/czg1225/SlimSAM) [@chen20230] which uses a fraction of the parameters and achieves similar results as the original SAM. 

## Introduction

### What is Object Detection?

Object detection is a computer vision task that involves identifying and locating objects within an image. It goes beyond simply classifying the entire image and instead identifies the specific locations and boundaries of individual objects.

### What is Instance Segmentation?

Instance segmentation is a more advanced computer vision task that builds upon object detection. It not only identifies the objects in an image but also accurately segments each object instance, providing a pixel-level mask for each detected object.

### What are Pretrained Models?

Pretrained models are machine learning models that have been trained on large datasets, typically for a specific task like object detection or instance segmentation. These models can then be fine-tuned or used directly for similar tasks, saving time and resources compared to training a model from scratch. Pretrained models are particularly useful in academic settings where we often have far less data at hand. Fortunately, for many computer vision tasks, the basic building blocks are going to be the same regardless of application. For example, detecting the edge of a car is not all that different from detecting the edge of a flower, since an edge is an edge.  

### Color Extraction Process

In addition to segmenting objects within an image, the SegmentR package also provides tools for analyzing the colors present in the segmented regions. This color extraction process allows users to gain insights into the visual characteristics of the detected objects.

## Installation

Installation
Using the SegmentR package requires both R and Python to be installed on your system, as well as the Conda package manager. This is because the package utilizes Python-based models and scripts for the image segmentation functionality.

The first step is to ensure you have R and Python installed. If you don't have Python installed, you can download it from the official Python website (https://www.python.org/downloads/). Once Python is installed, you'll need to install the Conda package manager, which you can download from the Anaconda website (https://www.anaconda.com/download).

After setting up the necessary dependencies, you can install the SegmentR package itself. The package provides a function called setup_conda_environment() that will handle the setup of the required Python environment and dependencies.

There are two options for setting up the Conda environment:

General Environment (Recommended):

Use the setup_conda_environment(`env_type = "general"`) function.
This will create a Conda environment with the necessary Python packages, but the specific versions may vary depending on what's available in the package repositories.

This option is recommended as it is more flexible and may be easier to maintain in the long run.

Specific Environment:

Use the setup_conda_environment(`env_type = "specific"`) function.

This will create a Conda environment with a specific set of package versions that are known to work with the SegmentR package.

This option is useful if you need to ensure the package works with a specific set of dependencies, but it may be more difficult to maintain over time as package versions are updated.

Once you've run the setup_conda_environment() function, the SegmentR package should be ready to use. You can then load the package in your R session and start exploring its functionality, as shown in the examples throughout the vignette.

## Loading Example Data

The SegmentR package comes with a dataset of 10 images from iNaturalist, 9 licensed under the CC0 (public domain) license and 1 licensed under the CC BY-NC. We can load this example data using the `load_segmentr_example_data()` function:

```{r}
library(SegmentR)
library(imager)
library(RColorBrewer)
example_data <- load_segmentr_example_data()
```

## Example 1

The first example will remove the fins of a fish from the whole fish. You begin by specifying the path to the image and the text prompts. Since I want to remove fins, I specify to look for fins in the image. I also want the whole fish, so I specify fish. 

```{r, eval=FALSE}
# not run in vignette building
results <- run_grounded_segmentation(example_data$image_paths_1, labels = c("a fish.", "a fin."))
```

The results object will contain the necessary meta data for downstream analysis. This includes paths to the json, image, etc. However, so that you don't have to run it every time to get those file paths, you can just specify the image location and generate the default path setup using get_segmentaiton_paths. This will return the same output as run_grounded_segmentation but without conducting any computationally demanding segmentations. We can then load the results.

```{r, eval=TRUE}
results <- get_segmentation_paths(example_data$image_paths_1)
seg_res <- load_segmentation_results(results$image_path)
```

We can plot our detections using plot_seg_results()

```{r, eval=TRUE}
plot_seg_results(seg_res, main = "")
```

So things look okay. We detected some fins, the whole body of the fish (hard to see because of overlap), but fins were detected as the entire fish. Clearly this is wrong and if we removed overlap based on this, the entire fish would be removed! So let's remove one of the masks and see if we can't improve the quality. 

```{r}
plot_seg_results(remove_mask(seg_res, 2), main = "")
```

Much better, from here we can reasign this variable and we export the results. 

```{r}
seg_res <- remove_mask(seg_res, 2)
# export_transparent_png(input = seg_res, output_path = "your_path_here", remove_overlap = TRUE, crop = TRUE)
# export_transparent_png(input = seg_res, prefix = "full_bream", output_path = "your_path_here", remove_overlap = FALSE, crop = TRUE)
```
